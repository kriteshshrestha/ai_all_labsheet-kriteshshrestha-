{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjrLOPwBSM5SwA9iM1xGSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kriteshshrestha/ai_lab_vaccume_knn-kriteshshrestha-/blob/main/perception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUA53emY1gHb",
        "outputId": "36834fe2-ef1d-479b-9344-ff5bda7ca127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron for AND gate:\n",
            "Initial weights: [-0.3635083366141705, 0.2492668162213061], Bias: -0.023\n",
            "Epoch Input1 Input2 Weight1 Weight2 Output Error ChangeW1 ChangeW2 NewW1   NewW2\n",
            "    1      0      0  -0.364   0.249      0     0    0.000    0.000  -0.364   0.249\n",
            "    1      0      1  -0.364   0.249      1    -1   -0.000   -0.100  -0.364   0.149\n",
            "    1      1      0  -0.364   0.149      0     0    0.000    0.000  -0.364   0.149\n",
            "    1      1      1  -0.364   0.149      0     1    0.100    0.100  -0.264   0.249\n",
            "    2      0      0  -0.264   0.249      0     0    0.000    0.000  -0.264   0.249\n",
            "    2      0      1  -0.264   0.249      1    -1   -0.000   -0.100  -0.264   0.149\n",
            "    2      1      0  -0.264   0.149      0     0    0.000    0.000  -0.264   0.149\n",
            "    2      1      1  -0.264   0.149      0     1    0.100    0.100  -0.164   0.249\n",
            "    3      0      0  -0.164   0.249      0     0    0.000    0.000  -0.164   0.249\n",
            "    3      0      1  -0.164   0.249      1    -1   -0.000   -0.100  -0.164   0.149\n",
            "    3      1      0  -0.164   0.149      0     0    0.000    0.000  -0.164   0.149\n",
            "    3      1      1  -0.164   0.149      0     1    0.100    0.100  -0.064   0.249\n",
            "    4      0      0  -0.064   0.249      0     0    0.000    0.000  -0.064   0.249\n",
            "    4      0      1  -0.064   0.249      1    -1   -0.000   -0.100  -0.064   0.149\n",
            "    4      1      0  -0.064   0.149      0     0    0.000    0.000  -0.064   0.149\n",
            "    4      1      1  -0.064   0.149      0     1    0.100    0.100   0.036   0.249\n",
            "    5      0      0   0.036   0.249      0     0    0.000    0.000   0.036   0.249\n",
            "    5      0      1   0.036   0.249      1    -1   -0.000   -0.100   0.036   0.149\n",
            "    5      1      0   0.036   0.149      0     0    0.000    0.000   0.036   0.149\n",
            "    5      1      1   0.036   0.149      1     0    0.000    0.000   0.036   0.149\n",
            "    6      0      0   0.036   0.149      0     0    0.000    0.000   0.036   0.149\n",
            "    6      0      1   0.036   0.149      1    -1   -0.000   -0.100   0.036   0.049\n",
            "    6      1      0   0.036   0.049      0     0    0.000    0.000   0.036   0.049\n",
            "    6      1      1   0.036   0.049      0     1    0.100    0.100   0.136   0.149\n",
            "    7      0      0   0.136   0.149      0     0    0.000    0.000   0.136   0.149\n",
            "    7      0      1   0.136   0.149      1    -1   -0.000   -0.100   0.136   0.049\n",
            "    7      1      0   0.136   0.049      0     0    0.000    0.000   0.136   0.049\n",
            "    7      1      1   0.136   0.049      0     1    0.100    0.100   0.236   0.149\n",
            "    8      0      0   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "    8      0      1   0.236   0.149      1    -1   -0.000   -0.100   0.236   0.049\n",
            "    8      1      0   0.236   0.049      1    -1   -0.100   -0.000   0.136   0.049\n",
            "    8      1      1   0.136   0.049      0     1    0.100    0.100   0.236   0.149\n",
            "    9      0      0   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "    9      0      1   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "    9      1      0   0.236   0.149      1    -1   -0.100   -0.000   0.136   0.149\n",
            "    9      1      1   0.136   0.149      0     1    0.100    0.100   0.236   0.249\n",
            "   10      0      0   0.236   0.249      0     0    0.000    0.000   0.236   0.249\n",
            "   10      0      1   0.236   0.249      1    -1   -0.000   -0.100   0.236   0.149\n",
            "   10      1      0   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "   10      1      1   0.236   0.149      1     0    0.000    0.000   0.236   0.149\n",
            "   11      0      0   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "   11      0      1   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "   11      1      0   0.236   0.149      0     0    0.000    0.000   0.236   0.149\n",
            "   11      1      1   0.236   0.149      1     0    0.000    0.000   0.236   0.149\n",
            "Final accuracy for AND gate: 1.00\n",
            "\n",
            "Training Perceptron for OR gate:\n",
            "Initial weights: [0.9142323491725262, 0.5241514022700944], Bias: -0.790\n",
            "Epoch Input1 Input2 Weight1 Weight2 Output Error ChangeW1 ChangeW2 NewW1   NewW2\n",
            "    1      0      0   0.914   0.524      0     0    0.000    0.000   0.914   0.524\n",
            "    1      0      1   0.914   0.524      0     1    0.000    0.100   0.914   0.624\n",
            "    1      1      0   0.914   0.624      1     0    0.000    0.000   0.914   0.624\n",
            "    1      1      1   0.914   0.624      1     0    0.000    0.000   0.914   0.624\n",
            "    2      0      0   0.914   0.624      0     0    0.000    0.000   0.914   0.624\n",
            "    2      0      1   0.914   0.624      0     1    0.000    0.100   0.914   0.724\n",
            "    2      1      0   0.914   0.724      1     0    0.000    0.000   0.914   0.724\n",
            "    2      1      1   0.914   0.724      1     0    0.000    0.000   0.914   0.724\n",
            "    3      0      0   0.914   0.724      0     0    0.000    0.000   0.914   0.724\n",
            "    3      0      1   0.914   0.724      1     0    0.000    0.000   0.914   0.724\n",
            "    3      1      0   0.914   0.724      1     0    0.000    0.000   0.914   0.724\n",
            "    3      1      1   0.914   0.724      1     0    0.000    0.000   0.914   0.724\n",
            "Final accuracy for OR gate: 1.00\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.1, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "        self.bias = random.uniform(-1, 1)\n",
        "\n",
        "    def step_activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        print(f\"Initial weights: {self.weights}, Bias: {self.bias:.3f}\")\n",
        "        print(\"Epoch Input1 Input2 Weight1 Weight2 Output Error ChangeW1 ChangeW2 NewW1   NewW2\")\n",
        "        for epoch in range(self.epochs):\n",
        "            errors = 0\n",
        "            for inp, target in zip(inputs, targets):\n",
        "                w1_before, w2_before = self.weights  # store current weights\n",
        "                summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "                output = self.step_activation(summation)\n",
        "                error = target - output\n",
        "\n",
        "                # calculate changes\n",
        "                change_w1 = self.lr * error * inp[0]\n",
        "                change_w2 = self.lr * error * inp[1]\n",
        "\n",
        "                # update weights\n",
        "                new_w1 = w1_before + change_w1\n",
        "                new_w2 = w2_before + change_w2\n",
        "                self.weights = [new_w1, new_w2]\n",
        "                self.bias += self.lr * error\n",
        "\n",
        "                if error != 0:\n",
        "                    errors += 1\n",
        "\n",
        "                print(f\"{epoch+1:5} {inp[0]:6} {inp[1]:6} {w1_before:7.3f} {w2_before:7.3f} \"\n",
        "                      f\"{output:6} {error:5} {change_w1:8.3f} {change_w2:8.3f} \"\n",
        "                      f\"{new_w1:7.3f} {new_w2:7.3f}\")\n",
        "            if errors == 0:\n",
        "                break\n",
        "\n",
        "    def evaluate(self, inputs, targets):\n",
        "        correct = 0\n",
        "        for inp, target in zip(inputs, targets):\n",
        "            summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "            prediction = self.step_activation(summation)\n",
        "            if prediction == target:\n",
        "                correct += 1\n",
        "        return correct / len(inputs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inputs = [[0,0], [0,1], [1,0], [1,1]]\n",
        "\n",
        "    print(\"Training Perceptron for AND gate:\")\n",
        "    targets_and = [0, 0, 0, 1]\n",
        "    perceptron_and = Perceptron(lr=0.1, epochs=100)\n",
        "    perceptron_and.train(inputs, targets_and)\n",
        "    acc_and = perceptron_and.evaluate(inputs, targets_and)\n",
        "    print(f\"Final accuracy for AND gate: {acc_and:.2f}\")\n",
        "\n",
        "    print(\"\\nTraining Perceptron for OR gate:\")\n",
        "    targets_or = [0, 1, 1, 1]\n",
        "    perceptron_or = Perceptron(lr=0.1, epochs=100)\n",
        "    perceptron_or.train(inputs, targets_or)\n",
        "    acc_or = perceptron_or.evaluate(inputs, targets_or)\n",
        "    print(f\"Final accuracy for OR gate: {acc_or:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]\n",
        "        self.bias = random.uniform(-1, 1)\n",
        "\n",
        "    def step_activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        for epoch in range(self.epochs):\n",
        "            errors = 0\n",
        "            for inp, target in zip(inputs, targets):\n",
        "                summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "                output = self.step_activation(summation)\n",
        "                error = target - output\n",
        "                if error != 0:\n",
        "                    errors += 1\n",
        "                    # Update weights and bias\n",
        "                    self.weights = [\n",
        "                        w + self.lr * error * i for w, i in zip(self.weights, inp)\n",
        "                    ]\n",
        "                    self.bias += self.lr * error\n",
        "            if errors == 0:\n",
        "                break  # stop if perfectly classified\n",
        "\n",
        "    def evaluate(self, inputs, targets):\n",
        "        correct = 0\n",
        "        for inp, target in zip(inputs, targets):\n",
        "            summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "            prediction = self.step_activation(summation)\n",
        "            if prediction == target:\n",
        "                correct += 1\n",
        "        return correct / len(inputs)\n",
        "\n",
        "def generate_truth_table(n):\n",
        "    \"\"\"Generates truth table with n inputs\"\"\"\n",
        "    return list(itertools.product([0, 1], repeat=n))\n",
        "\n",
        "def generate_targets(inputs, gate_type):\n",
        "    \"\"\"Creates target outputs for AND or OR gates\"\"\"\n",
        "    if gate_type == 'AND':\n",
        "        return [int(all(inp)) for inp in inputs]\n",
        "    elif gate_type == 'OR':\n",
        "        return [int(any(inp)) for inp in inputs]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid gate_type: choose 'AND' or 'OR'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for n in [3, 4]:\n",
        "        print(f\"\\n--- Training Perceptron for {n}-input AND gate ---\")\n",
        "        inputs = generate_truth_table(n)\n",
        "        targets_and = generate_targets(inputs, 'AND')\n",
        "        perceptron_and = Perceptron(input_size=n, lr=0.1, epochs=100)\n",
        "        perceptron_and.train(inputs, targets_and)\n",
        "        accuracy_and = perceptron_and.evaluate(inputs, targets_and)\n",
        "        print(f\"Final weights: {perceptron_and.weights}\")\n",
        "        print(f\"Final bias: {perceptron_and.bias:.3f}\")\n",
        "        print(f\"Accuracy: {accuracy_and:.2f}\")\n",
        "\n",
        "        print(f\"\\n--- Training Perceptron for {n}-input OR gate ---\")\n",
        "        targets_or = generate_targets(inputs, 'OR')\n",
        "        perceptron_or = Perceptron(input_size=n, lr=0.1, epochs=100)\n",
        "        perceptron_or.train(inputs, targets_or)\n",
        "        accuracy_or = perceptron_or.evaluate(inputs, targets_or)\n",
        "        print(f\"Final weights: {perceptron_or.weights}\")\n",
        "        print(f\"Final bias: {perceptron_or.bias:.3f}\")\n",
        "        print(f\"Accuracy: {accuracy_or:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dqrCS7L1ktG",
        "outputId": "9b15f03d-a314-4b52-ce6f-c387915bdc8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Perceptron for 3-input AND gate ---\n",
            "Final weights: [0.26107128492317966, 0.13910883732011586, 0.1266450750707917]\n",
            "Final bias: -0.517\n",
            "Accuracy: 1.00\n",
            "\n",
            "--- Training Perceptron for 3-input OR gate ---\n",
            "Final weights: [0.676933394525856, 0.8081464964602241, 0.9579513962618752]\n",
            "Final bias: -0.582\n",
            "Accuracy: 1.00\n",
            "\n",
            "--- Training Perceptron for 4-input AND gate ---\n",
            "Final weights: [0.4593444604916028, 0.3281409817007648, 0.2500526053333455, 0.15476107787916074]\n",
            "Final bias: -1.127\n",
            "Accuracy: 1.00\n",
            "\n",
            "--- Training Perceptron for 4-input OR gate ---\n",
            "Final weights: [0.10613324230829033, 0.46469890780586887, 0.14771444704319597, 0.41750395929108597]\n",
            "Final bias: -0.015\n",
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class LinearPerceptron:\n",
        "    def __init__(self, input_size, lr=0.01, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]\n",
        "        self.bias = random.uniform(-1, 1)\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for inp, target in zip(inputs, targets):\n",
        "                # Linear activation: output = weighted sum + bias\n",
        "                output = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "                error = target - output\n",
        "                total_error += error ** 2\n",
        "                # Update weights and bias\n",
        "                self.weights = [w + self.lr * error * i for w, i in zip(self.weights, inp)]\n",
        "                self.bias += self.lr * error\n",
        "            mse = total_error / len(inputs)\n",
        "            print(f\"Epoch {epoch+1:3}: MSE = {mse:.4f}\")\n",
        "        print(f\"\\nFinal learned weights: {self.weights}\")\n",
        "        print(f\"Final learned bias: {self.bias:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(42)  # For reproducibility\n",
        "\n",
        "    # Generate 10 random samples: x1, x2, x3 in [0,1]\n",
        "    inputs = [[random.random(), random.random(), random.random()] for _ in range(10)]\n",
        "\n",
        "    # Compute targets: y = 2x1 + 3x2 - x3 + 5\n",
        "    targets = [2*x[0] + 3*x[1] - x[2] + 5 for x in inputs]\n",
        "\n",
        "    perceptron = LinearPerceptron(input_size=3, lr=0.01, epochs=100)\n",
        "    perceptron.train(inputs, targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mJDcFci1scM",
        "outputId": "30b638c6-344d-467e-b352-47b81581d803"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1: MSE = 22.2414\n",
            "Epoch   2: MSE = 16.3011\n",
            "Epoch   3: MSE = 11.9773\n",
            "Epoch   4: MSE = 8.8299\n",
            "Epoch   5: MSE = 6.5388\n",
            "Epoch   6: MSE = 4.8708\n",
            "Epoch   7: MSE = 3.6562\n",
            "Epoch   8: MSE = 2.7717\n",
            "Epoch   9: MSE = 2.1272\n",
            "Epoch  10: MSE = 1.6575\n",
            "Epoch  11: MSE = 1.3149\n",
            "Epoch  12: MSE = 1.0647\n",
            "Epoch  13: MSE = 0.8818\n",
            "Epoch  14: MSE = 0.7479\n",
            "Epoch  15: MSE = 0.6495\n",
            "Epoch  16: MSE = 0.5770\n",
            "Epoch  17: MSE = 0.5234\n",
            "Epoch  18: MSE = 0.4834\n",
            "Epoch  19: MSE = 0.4534\n",
            "Epoch  20: MSE = 0.4306\n",
            "Epoch  21: MSE = 0.4131\n",
            "Epoch  22: MSE = 0.3994\n",
            "Epoch  23: MSE = 0.3885\n",
            "Epoch  24: MSE = 0.3796\n",
            "Epoch  25: MSE = 0.3722\n",
            "Epoch  26: MSE = 0.3659\n",
            "Epoch  27: MSE = 0.3604\n",
            "Epoch  28: MSE = 0.3554\n",
            "Epoch  29: MSE = 0.3509\n",
            "Epoch  30: MSE = 0.3467\n",
            "Epoch  31: MSE = 0.3428\n",
            "Epoch  32: MSE = 0.3390\n",
            "Epoch  33: MSE = 0.3354\n",
            "Epoch  34: MSE = 0.3318\n",
            "Epoch  35: MSE = 0.3284\n",
            "Epoch  36: MSE = 0.3250\n",
            "Epoch  37: MSE = 0.3217\n",
            "Epoch  38: MSE = 0.3185\n",
            "Epoch  39: MSE = 0.3153\n",
            "Epoch  40: MSE = 0.3121\n",
            "Epoch  41: MSE = 0.3090\n",
            "Epoch  42: MSE = 0.3059\n",
            "Epoch  43: MSE = 0.3028\n",
            "Epoch  44: MSE = 0.2998\n",
            "Epoch  45: MSE = 0.2968\n",
            "Epoch  46: MSE = 0.2939\n",
            "Epoch  47: MSE = 0.2909\n",
            "Epoch  48: MSE = 0.2880\n",
            "Epoch  49: MSE = 0.2852\n",
            "Epoch  50: MSE = 0.2823\n",
            "Epoch  51: MSE = 0.2795\n",
            "Epoch  52: MSE = 0.2768\n",
            "Epoch  53: MSE = 0.2740\n",
            "Epoch  54: MSE = 0.2713\n",
            "Epoch  55: MSE = 0.2686\n",
            "Epoch  56: MSE = 0.2659\n",
            "Epoch  57: MSE = 0.2633\n",
            "Epoch  58: MSE = 0.2607\n",
            "Epoch  59: MSE = 0.2581\n",
            "Epoch  60: MSE = 0.2556\n",
            "Epoch  61: MSE = 0.2531\n",
            "Epoch  62: MSE = 0.2506\n",
            "Epoch  63: MSE = 0.2481\n",
            "Epoch  64: MSE = 0.2457\n",
            "Epoch  65: MSE = 0.2432\n",
            "Epoch  66: MSE = 0.2408\n",
            "Epoch  67: MSE = 0.2385\n",
            "Epoch  68: MSE = 0.2361\n",
            "Epoch  69: MSE = 0.2338\n",
            "Epoch  70: MSE = 0.2315\n",
            "Epoch  71: MSE = 0.2293\n",
            "Epoch  72: MSE = 0.2270\n",
            "Epoch  73: MSE = 0.2248\n",
            "Epoch  74: MSE = 0.2226\n",
            "Epoch  75: MSE = 0.2204\n",
            "Epoch  76: MSE = 0.2183\n",
            "Epoch  77: MSE = 0.2162\n",
            "Epoch  78: MSE = 0.2140\n",
            "Epoch  79: MSE = 0.2120\n",
            "Epoch  80: MSE = 0.2099\n",
            "Epoch  81: MSE = 0.2079\n",
            "Epoch  82: MSE = 0.2058\n",
            "Epoch  83: MSE = 0.2038\n",
            "Epoch  84: MSE = 0.2019\n",
            "Epoch  85: MSE = 0.1999\n",
            "Epoch  86: MSE = 0.1980\n",
            "Epoch  87: MSE = 0.1961\n",
            "Epoch  88: MSE = 0.1942\n",
            "Epoch  89: MSE = 0.1923\n",
            "Epoch  90: MSE = 0.1904\n",
            "Epoch  91: MSE = 0.1886\n",
            "Epoch  92: MSE = 0.1868\n",
            "Epoch  93: MSE = 0.1850\n",
            "Epoch  94: MSE = 0.1832\n",
            "Epoch  95: MSE = 0.1815\n",
            "Epoch  96: MSE = 0.1797\n",
            "Epoch  97: MSE = 0.1780\n",
            "Epoch  98: MSE = 0.1763\n",
            "Epoch  99: MSE = 0.1746\n",
            "Epoch 100: MSE = 0.1729\n",
            "\n",
            "Final learned weights: [2.2982049457191276, 2.2367794920342505, 0.7558416194255322]\n",
            "Final learned bias: 4.238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class LinearPerceptron:\n",
        "    def __init__(self, input_size, lr=0.01, epochs=100):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        # Initialize weights and bias randomly\n",
        "        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]\n",
        "        self.bias = random.uniform(-1, 1)\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        for epoch in range(1, self.epochs+1):\n",
        "            total_error = 0\n",
        "            for inp, target in zip(inputs, targets):\n",
        "                output = sum(w * i for w, i in zip(self.weights, inp)) + self.bias\n",
        "                error = target - output\n",
        "                total_error += error ** 2\n",
        "                # Update weights and bias\n",
        "                self.weights = [w + self.lr * error * i for w, i in zip(self.weights, inp)]\n",
        "                self.bias += self.lr * error\n",
        "            mse = total_error / len(inputs)\n",
        "            print(f\"Epoch {epoch:3}: MSE = {mse:.4f}\")\n",
        "        print(f\"Final learned weights: {[round(w, 4) for w in self.weights]}\")\n",
        "        print(f\"Final learned bias: {round(self.bias, 4)}\")\n",
        "\n",
        "def generate_dataset(n, num_samples=10):\n",
        "    # Random true weights and bias=5\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n)]\n",
        "    bias = 5\n",
        "    # Generate random inputs and compute targets\n",
        "    inputs = [[random.random() for _ in range(n)] for _ in range(num_samples)]\n",
        "    targets = [sum(w * x for w, x in zip(true_weights, sample)) + bias for sample in inputs]\n",
        "    print(f\"True weights: {[round(w, 4) for w in true_weights]}, True bias: {bias}\")\n",
        "    return inputs, targets\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(42)  # for reproducibility\n",
        "\n",
        "    for n in [4, 5]:\n",
        "        print(f\"\\n=== Training Linear Perceptron with n={n} features ===\")\n",
        "        inputs, targets = generate_dataset(n)\n",
        "        perceptron = LinearPerceptron(input_size=n, lr=0.01, epochs=100)\n",
        "        perceptron.train(inputs, targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQiKH8Vu11AH",
        "outputId": "d226a3ad-27fd-4969-8a34-1228096059eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Linear Perceptron with n=4 features ===\n",
            "True weights: [0.2789, -0.95, -0.4499, -0.5536], True bias: 5\n",
            "Epoch   1: MSE = 24.2228\n",
            "Epoch   2: MSE = 16.1910\n",
            "Epoch   3: MSE = 10.8896\n",
            "Epoch   4: MSE = 7.3890\n",
            "Epoch   5: MSE = 5.0764\n",
            "Epoch   6: MSE = 3.5474\n",
            "Epoch   7: MSE = 2.5355\n",
            "Epoch   8: MSE = 1.8648\n",
            "Epoch   9: MSE = 1.4192\n",
            "Epoch  10: MSE = 1.1222\n",
            "Epoch  11: MSE = 0.9235\n",
            "Epoch  12: MSE = 0.7895\n",
            "Epoch  13: MSE = 0.6984\n",
            "Epoch  14: MSE = 0.6357\n",
            "Epoch  15: MSE = 0.5918\n",
            "Epoch  16: MSE = 0.5603\n",
            "Epoch  17: MSE = 0.5371\n",
            "Epoch  18: MSE = 0.5194\n",
            "Epoch  19: MSE = 0.5054\n",
            "Epoch  20: MSE = 0.4939\n",
            "Epoch  21: MSE = 0.4841\n",
            "Epoch  22: MSE = 0.4754\n",
            "Epoch  23: MSE = 0.4675\n",
            "Epoch  24: MSE = 0.4602\n",
            "Epoch  25: MSE = 0.4533\n",
            "Epoch  26: MSE = 0.4467\n",
            "Epoch  27: MSE = 0.4404\n",
            "Epoch  28: MSE = 0.4342\n",
            "Epoch  29: MSE = 0.4282\n",
            "Epoch  30: MSE = 0.4223\n",
            "Epoch  31: MSE = 0.4166\n",
            "Epoch  32: MSE = 0.4109\n",
            "Epoch  33: MSE = 0.4054\n",
            "Epoch  34: MSE = 0.4000\n",
            "Epoch  35: MSE = 0.3946\n",
            "Epoch  36: MSE = 0.3893\n",
            "Epoch  37: MSE = 0.3842\n",
            "Epoch  38: MSE = 0.3791\n",
            "Epoch  39: MSE = 0.3740\n",
            "Epoch  40: MSE = 0.3691\n",
            "Epoch  41: MSE = 0.3642\n",
            "Epoch  42: MSE = 0.3594\n",
            "Epoch  43: MSE = 0.3547\n",
            "Epoch  44: MSE = 0.3501\n",
            "Epoch  45: MSE = 0.3455\n",
            "Epoch  46: MSE = 0.3410\n",
            "Epoch  47: MSE = 0.3365\n",
            "Epoch  48: MSE = 0.3322\n",
            "Epoch  49: MSE = 0.3278\n",
            "Epoch  50: MSE = 0.3236\n",
            "Epoch  51: MSE = 0.3194\n",
            "Epoch  52: MSE = 0.3153\n",
            "Epoch  53: MSE = 0.3112\n",
            "Epoch  54: MSE = 0.3072\n",
            "Epoch  55: MSE = 0.3033\n",
            "Epoch  56: MSE = 0.2994\n",
            "Epoch  57: MSE = 0.2956\n",
            "Epoch  58: MSE = 0.2918\n",
            "Epoch  59: MSE = 0.2881\n",
            "Epoch  60: MSE = 0.2844\n",
            "Epoch  61: MSE = 0.2808\n",
            "Epoch  62: MSE = 0.2773\n",
            "Epoch  63: MSE = 0.2737\n",
            "Epoch  64: MSE = 0.2703\n",
            "Epoch  65: MSE = 0.2669\n",
            "Epoch  66: MSE = 0.2635\n",
            "Epoch  67: MSE = 0.2602\n",
            "Epoch  68: MSE = 0.2570\n",
            "Epoch  69: MSE = 0.2538\n",
            "Epoch  70: MSE = 0.2506\n",
            "Epoch  71: MSE = 0.2475\n",
            "Epoch  72: MSE = 0.2444\n",
            "Epoch  73: MSE = 0.2414\n",
            "Epoch  74: MSE = 0.2384\n",
            "Epoch  75: MSE = 0.2354\n",
            "Epoch  76: MSE = 0.2325\n",
            "Epoch  77: MSE = 0.2297\n",
            "Epoch  78: MSE = 0.2268\n",
            "Epoch  79: MSE = 0.2241\n",
            "Epoch  80: MSE = 0.2213\n",
            "Epoch  81: MSE = 0.2186\n",
            "Epoch  82: MSE = 0.2159\n",
            "Epoch  83: MSE = 0.2133\n",
            "Epoch  84: MSE = 0.2107\n",
            "Epoch  85: MSE = 0.2082\n",
            "Epoch  86: MSE = 0.2056\n",
            "Epoch  87: MSE = 0.2032\n",
            "Epoch  88: MSE = 0.2007\n",
            "Epoch  89: MSE = 0.1983\n",
            "Epoch  90: MSE = 0.1959\n",
            "Epoch  91: MSE = 0.1936\n",
            "Epoch  92: MSE = 0.1912\n",
            "Epoch  93: MSE = 0.1890\n",
            "Epoch  94: MSE = 0.1867\n",
            "Epoch  95: MSE = 0.1845\n",
            "Epoch  96: MSE = 0.1823\n",
            "Epoch  97: MSE = 0.1801\n",
            "Epoch  98: MSE = 0.1780\n",
            "Epoch  99: MSE = 0.1759\n",
            "Epoch 100: MSE = 0.1738\n",
            "Final learned weights: [0.7862, -0.1936, 0.0313, 0.2241]\n",
            "Final learned bias: 3.7266\n",
            "\n",
            "=== Training Linear Perceptron with n=5 features ===\n",
            "True weights: [-0.2703, -0.2596, -0.581, -0.466, 0.8733], True bias: 5\n",
            "Epoch   1: MSE = 15.4103\n",
            "Epoch   2: MSE = 9.5656\n",
            "Epoch   3: MSE = 5.9512\n",
            "Epoch   4: MSE = 3.7165\n",
            "Epoch   5: MSE = 2.3353\n",
            "Epoch   6: MSE = 1.4818\n",
            "Epoch   7: MSE = 0.9547\n",
            "Epoch   8: MSE = 0.6292\n",
            "Epoch   9: MSE = 0.4284\n",
            "Epoch  10: MSE = 0.3046\n",
            "Epoch  11: MSE = 0.2283\n",
            "Epoch  12: MSE = 0.1812\n",
            "Epoch  13: MSE = 0.1523\n",
            "Epoch  14: MSE = 0.1345\n",
            "Epoch  15: MSE = 0.1235\n",
            "Epoch  16: MSE = 0.1167\n",
            "Epoch  17: MSE = 0.1124\n",
            "Epoch  18: MSE = 0.1098\n",
            "Epoch  19: MSE = 0.1080\n",
            "Epoch  20: MSE = 0.1069\n",
            "Epoch  21: MSE = 0.1061\n",
            "Epoch  22: MSE = 0.1055\n",
            "Epoch  23: MSE = 0.1050\n",
            "Epoch  24: MSE = 0.1046\n",
            "Epoch  25: MSE = 0.1042\n",
            "Epoch  26: MSE = 0.1039\n",
            "Epoch  27: MSE = 0.1036\n",
            "Epoch  28: MSE = 0.1032\n",
            "Epoch  29: MSE = 0.1029\n",
            "Epoch  30: MSE = 0.1026\n",
            "Epoch  31: MSE = 0.1023\n",
            "Epoch  32: MSE = 0.1019\n",
            "Epoch  33: MSE = 0.1016\n",
            "Epoch  34: MSE = 0.1013\n",
            "Epoch  35: MSE = 0.1010\n",
            "Epoch  36: MSE = 0.1006\n",
            "Epoch  37: MSE = 0.1003\n",
            "Epoch  38: MSE = 0.1000\n",
            "Epoch  39: MSE = 0.0997\n",
            "Epoch  40: MSE = 0.0994\n",
            "Epoch  41: MSE = 0.0990\n",
            "Epoch  42: MSE = 0.0987\n",
            "Epoch  43: MSE = 0.0984\n",
            "Epoch  44: MSE = 0.0981\n",
            "Epoch  45: MSE = 0.0978\n",
            "Epoch  46: MSE = 0.0975\n",
            "Epoch  47: MSE = 0.0972\n",
            "Epoch  48: MSE = 0.0969\n",
            "Epoch  49: MSE = 0.0966\n",
            "Epoch  50: MSE = 0.0963\n",
            "Epoch  51: MSE = 0.0960\n",
            "Epoch  52: MSE = 0.0957\n",
            "Epoch  53: MSE = 0.0954\n",
            "Epoch  54: MSE = 0.0951\n",
            "Epoch  55: MSE = 0.0948\n",
            "Epoch  56: MSE = 0.0945\n",
            "Epoch  57: MSE = 0.0942\n",
            "Epoch  58: MSE = 0.0940\n",
            "Epoch  59: MSE = 0.0937\n",
            "Epoch  60: MSE = 0.0934\n",
            "Epoch  61: MSE = 0.0931\n",
            "Epoch  62: MSE = 0.0928\n",
            "Epoch  63: MSE = 0.0925\n",
            "Epoch  64: MSE = 0.0923\n",
            "Epoch  65: MSE = 0.0920\n",
            "Epoch  66: MSE = 0.0917\n",
            "Epoch  67: MSE = 0.0914\n",
            "Epoch  68: MSE = 0.0912\n",
            "Epoch  69: MSE = 0.0909\n",
            "Epoch  70: MSE = 0.0906\n",
            "Epoch  71: MSE = 0.0904\n",
            "Epoch  72: MSE = 0.0901\n",
            "Epoch  73: MSE = 0.0898\n",
            "Epoch  74: MSE = 0.0896\n",
            "Epoch  75: MSE = 0.0893\n",
            "Epoch  76: MSE = 0.0891\n",
            "Epoch  77: MSE = 0.0888\n",
            "Epoch  78: MSE = 0.0885\n",
            "Epoch  79: MSE = 0.0883\n",
            "Epoch  80: MSE = 0.0880\n",
            "Epoch  81: MSE = 0.0878\n",
            "Epoch  82: MSE = 0.0875\n",
            "Epoch  83: MSE = 0.0873\n",
            "Epoch  84: MSE = 0.0870\n",
            "Epoch  85: MSE = 0.0868\n",
            "Epoch  86: MSE = 0.0865\n",
            "Epoch  87: MSE = 0.0863\n",
            "Epoch  88: MSE = 0.0860\n",
            "Epoch  89: MSE = 0.0858\n",
            "Epoch  90: MSE = 0.0856\n",
            "Epoch  91: MSE = 0.0853\n",
            "Epoch  92: MSE = 0.0851\n",
            "Epoch  93: MSE = 0.0848\n",
            "Epoch  94: MSE = 0.0846\n",
            "Epoch  95: MSE = 0.0844\n",
            "Epoch  96: MSE = 0.0841\n",
            "Epoch  97: MSE = 0.0839\n",
            "Epoch  98: MSE = 0.0837\n",
            "Epoch  99: MSE = 0.0834\n",
            "Epoch 100: MSE = 0.0832\n",
            "Final learned weights: [0.3896, 0.8918, 0.2311, 0.8165, 0.9123]\n",
            "Final learned bias: 2.9256\n"
          ]
        }
      ]
    }
  ]
}